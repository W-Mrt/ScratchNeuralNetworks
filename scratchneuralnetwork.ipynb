{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0JEwSrvkmre6BdadtEjo6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/W-Mrt/ScratchNeuralNetworks/blob/main/scratchneuralnetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zyDltIqMvzjX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from math import erf\n",
        "from matplotlib import pyplot as plt\n",
        "from abc import ABC, abstractmethod"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import data from Kaggle\n",
        "import json, os\n",
        "\n",
        "# 1. Install the Kaggle API client\n",
        "!pip install kaggle --quiet"
      ],
      "metadata": {
        "id": "0hfyL3wZwDzQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Set your Kaggle credentials\n",
        "# Load the file\n",
        "with open('kaggle.json') as f:\n",
        "    creds = json.load(f)\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['KAGGLE_USERNAME'] = creds['username']\n",
        "os.environ['KAGGLE_KEY'] = creds['key']\n",
        "\n",
        "# 3. Import and authenticate\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "api = KaggleApi()\n",
        "api.authenticate()  # reads env variables correctly\n",
        "\n",
        "# 4. Download and unzip the MNIST CSV dataset\n",
        "api.dataset_download_files(\n",
        "    'oddrationale/mnist-in-csv',\n",
        "    path='mnist-data',\n",
        "    unzip=True,\n",
        "    quiet=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPVmINn-ulxB",
        "outputId": "a46ba35d-2f0b-4b9f-eb18-a0c2879ada76"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/oddrationale/mnist-in-csv\n",
            "Downloading mnist-in-csv.zip to mnist-data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15.2M/15.2M [00:00<00:00, 910MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load CSVs into pandas and prepare NumPy arrays\n",
        "\n",
        "train = pd.read_csv('mnist-data/mnist_train.csv')\n",
        "test = pd.read_csv('mnist-data/mnist_test.csv')\n",
        "\n",
        "# 2. Concatenate data\n",
        "full = pd.concat([train, test], ignore_index=True)\n",
        "\n",
        "# 3. Shuffle dataset\n",
        "full = full.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# 4. Split into new train/test (e.g., 80/20)\n",
        "split = int(0.8 * len(full))\n",
        "train_new = full.iloc[:split]\n",
        "test_new  = full.iloc[split:]\n",
        "\n",
        "# 5. Extract features and labels\n",
        "X_train = train_new.drop('label', axis=1).values.astype(np.float32) / 255.0\n",
        "Y_train = train_new['label'].values.astype(np.int64)\n",
        "X_test  = test_new.drop('label', axis=1).values.astype(np.float32) / 255.0\n",
        "Y_test  = test_new['label'].values.astype(np.int64)\n",
        "\n",
        "# Optional - Transpose for NumPy network\n",
        "X_train = X_train.T         # (784, 60000)\n",
        "Y_train = Y_train.reshape(1, -1)\n",
        "X_test = X_test.T           # (784, 10000)\n",
        "Y_test = Y_test.reshape(1, -1)"
      ],
      "metadata": {
        "id": "JY6J1ZaPydm_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define NN model and utilities\n",
        "\n",
        "# --- Initialization ---\n",
        "def init_params(input_dim=784, hidden_dim=64, output_dim=10):\n",
        "    np.random.seed(1)\n",
        "    W1 = np.random.randn(hidden_dim, input_dim) * 0.01\n",
        "    b1 = np.zeros((hidden_dim, 1))\n",
        "    W2 = np.random.randn(output_dim, hidden_dim) * 0.01\n",
        "    b2 = np.zeros((output_dim, 1))\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "# --- Activations ---\n",
        "def relu(Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "def drelu(Z):\n",
        "    return (Z > 0).astype(float)\n",
        "\n",
        "def softmax(Z):\n",
        "    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
        "    return expZ / expZ.sum(axis=0, keepdims=True)\n",
        "\n",
        "# --- One-hot encoding ---\n",
        "def one_hot(Y, num_classes=10):\n",
        "    m = Y.size\n",
        "    Y_oh = np.zeros((num_classes, m))\n",
        "    Y_oh[Y, np.arange(m)] = 1\n",
        "    return Y_oh\n",
        "\n",
        "# --- Forward pass ---\n",
        "def forward(X, W1, b1, W2, b2):\n",
        "    Z1 = W1.dot(X) + b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    return Z1, A1, Z2, A2\n",
        "\n",
        "# --- Loss ---\n",
        "def compute_loss(A2, Y_oh):\n",
        "    m = Y_oh.shape[1]\n",
        "    loss = -np.sum(Y_oh * np.log(A2 + 1e-9)) / m\n",
        "    return loss\n",
        "\n",
        "# --- Backward pass ---\n",
        "def backward(X, Y_oh, Z1, A1, A2, W2):\n",
        "    m = X.shape[1]\n",
        "    dZ2 = A2 - Y_oh\n",
        "    dW2 = (1/m) * dZ2.dot(A1.T)\n",
        "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "    dA1 = W2.T.dot(dZ2)\n",
        "    dZ1 = dA1 * drelu(Z1)\n",
        "    dW1 = (1/m) * dZ1.dot(X.T)\n",
        "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "# --- Parameter update ---\n",
        "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, lr):\n",
        "    W1 -= lr * dW1\n",
        "    b1 -= lr * db1\n",
        "    W2 -= lr * dW2\n",
        "    b2 -= lr * db2\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "# --- Prediction ---\n",
        "def predict(X, W1, b1, W2, b2):\n",
        "    _, _, _, A2 = forward(X, W1, b1, W2, b2)\n",
        "    return np.argmax(A2, axis=0)"
      ],
      "metadata": {
        "id": "rBlQdjrw2smD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X, Y, epochs=30, lr=0.1, hidden_dim=64):\n",
        "    n_x, m = X.shape\n",
        "    Y_oh = one_hot(Y.flatten(), num_classes=10)\n",
        "\n",
        "    #---- GRADIENT DESCENT\n",
        "    W1, b1, W2, b2 = init_params(input_dim=n_x, hidden_dim=hidden_dim)\n",
        "    for epoch in range(1, epochs+1):\n",
        "        Z1, A1, Z2, A2 = forward(X, W1, b1, W2, b2)\n",
        "        loss = compute_loss(A2, Y_oh)\n",
        "\n",
        "        dW1, db1, dW2, db2 = backward(X, Y_oh, Z1, A1, A2, W2)\n",
        "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, lr)\n",
        "\n",
        "    #---- show stats epoch, loss, accuracy\n",
        "        if epoch % 5 == 0 or epoch == 1:\n",
        "            preds = predict(X, W1, b1, W2, b2)\n",
        "            acc = np.mean(preds == Y.flatten())\n",
        "            print(f\"Epoch {epoch:2d} │ Loss: {loss:.4f} │ Accuracy: {acc*100:.2f}%\")\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "# Train the model\n",
        "W1, b1, W2, b2 = train(X_train, Y_train, epochs=100, lr=0.1, hidden_dim=64)\n",
        "\n",
        "# Evaluate on test set\n",
        "preds_test = predict(X_test, W1, b1, W2, b2)\n",
        "test_acc = np.mean(preds_test == Y_test.flatten())\n",
        "print(f\"\\nTest set accuracy: {test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "iUxoiCub3A-k",
        "outputId": "b879a7c1-1f56-4394-b2b3-bec6ebc6a688",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1 │ Loss: 2.3024 │ Accuracy: 10.44%\n",
            "Epoch  5 │ Loss: 2.2999 │ Accuracy: 25.47%\n",
            "Epoch 10 │ Loss: 2.2963 │ Accuracy: 42.68%\n",
            "Epoch 15 │ Loss: 2.2914 │ Accuracy: 53.23%\n",
            "Epoch 20 │ Loss: 2.2845 │ Accuracy: 58.30%\n",
            "Epoch 25 │ Loss: 2.2744 │ Accuracy: 59.77%\n",
            "Epoch 30 │ Loss: 2.2596 │ Accuracy: 59.64%\n",
            "Epoch 35 │ Loss: 2.2379 │ Accuracy: 58.50%\n",
            "Epoch 40 │ Loss: 2.2065 │ Accuracy: 57.39%\n",
            "Epoch 45 │ Loss: 2.1624 │ Accuracy: 57.41%\n",
            "Epoch 50 │ Loss: 2.1028 │ Accuracy: 59.05%\n",
            "Epoch 55 │ Loss: 2.0259 │ Accuracy: 62.08%\n",
            "Epoch 60 │ Loss: 1.9316 │ Accuracy: 65.35%\n",
            "Epoch 65 │ Loss: 1.8216 │ Accuracy: 68.08%\n",
            "Epoch 70 │ Loss: 1.7001 │ Accuracy: 69.63%\n",
            "Epoch 75 │ Loss: 1.5732 │ Accuracy: 70.31%\n",
            "Epoch 80 │ Loss: 1.4482 │ Accuracy: 70.85%\n",
            "Epoch 85 │ Loss: 1.3314 │ Accuracy: 71.81%\n",
            "Epoch 90 │ Loss: 1.2269 │ Accuracy: 73.14%\n",
            "Epoch 95 │ Loss: 1.1359 │ Accuracy: 74.68%\n",
            "Epoch 100 │ Loss: 1.0577 │ Accuracy: 76.09%\n",
            "\n",
            "Test set accuracy: 76.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Initializers:\n",
        "    \"\"\"Weight initialization strategies for dense layers.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def xavier_uniform(shape):\n",
        "        fan_in, fan_out = shape[1], shape[0]\n",
        "        limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
        "        return np.random.uniform(-limit, limit, size=shape)\n",
        "\n",
        "    @staticmethod\n",
        "    def xavier_normal(shape):\n",
        "        fan_in, fan_out = shape[1], shape[0]\n",
        "        std = np.sqrt(2.0 / (fan_in + fan_out))\n",
        "        return np.random.randn(*shape) * std\n",
        "\n",
        "    @staticmethod\n",
        "    def he_normal(shape):\n",
        "        fan_in = shape[1]\n",
        "        std = np.sqrt(2.0 / fan_in)\n",
        "        return np.random.randn(*shape) * std\n",
        "\n",
        "    @staticmethod\n",
        "    def lecun_normal(shape):\n",
        "        \"\"\"Designed for SELU activation: variance = 1/fan_in.\"\"\"\n",
        "        fan_in = shape[1]\n",
        "        std = np.sqrt(1.0 / fan_in)\n",
        "        return np.random.randn(*shape) * std\n",
        "\n",
        "    @staticmethod\n",
        "    def orthogonal(shape):\n",
        "        \"\"\"Orthogonal initialization via SVD/X decomposition.\"\"\"\n",
        "        a = np.random.randn(*shape)\n",
        "        u, _, v = np.linalg.svd(a, full_matrices=False)\n",
        "        return u.reshape(shape) if u.shape == shape else v.reshape(shape)\n",
        "\n",
        "    @staticmethod\n",
        "    def fixup_linear(shape, layer_idx, total_layers, m=2):\n",
        "        \"\"\"\n",
        "        Fixup initialization for a linear (dense) layer in ResNet-like blocks.\n",
        "        - Zero init for classification layer and last layer in each residual branch.\n",
        "        - He init scaled by L^(-1/(2m−2)) for other layers in residual branches.\n",
        "        - m = number of weight layers in a block/residual branch (usually 2).\n",
        "        \"\"\"\n",
        "        fan_in = shape[1]\n",
        "        if layer_idx == total_layers - 1:\n",
        "            # Zero the final linear layer or last layer in residual branch\n",
        "            return np.zeros(shape)\n",
        "        # He init base\n",
        "        W = np.random.randn(*shape) * np.sqrt(2.0 / fan_in)\n",
        "        # Scale according to Fixup theory: L^(−1/(2m−2))\n",
        "        scale = total_layers ** (-1.0 / (2 * m - 2))\n",
        "        return W * scale"
      ],
      "metadata": {
        "id": "tcyxjPW4BmSc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActivationFunctions:\n",
        "    \"\"\"Common activation functions and their derivatives.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(Z):\n",
        "        A = 1 / (1 + np.exp(-Z))\n",
        "        return A\n",
        "\n",
        "    @staticmethod\n",
        "    def dsigmoid(Z):\n",
        "        A = ActivationFunctions.sigmoid(Z)\n",
        "        return A * (1 - A)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(Z):\n",
        "        return np.tanh(Z)\n",
        "\n",
        "    @staticmethod\n",
        "    def dtanh(Z):\n",
        "        return 1 - np.tanh(Z) ** 2\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(Z):\n",
        "        return np.maximum(0, Z)\n",
        "\n",
        "    @staticmethod\n",
        "    def drelu(Z):\n",
        "        return (Z > 0).astype(float)\n",
        "\n",
        "    @staticmethod\n",
        "    def leaky_relu(Z, alpha=0.01):\n",
        "        return np.where(Z > 0, Z, alpha * Z)\n",
        "\n",
        "    @staticmethod\n",
        "    def dleaky_relu(Z, alpha=0.01):\n",
        "        return np.where(Z > 0, 1, alpha)\n",
        "\n",
        "    @staticmethod\n",
        "    def elu(Z, alpha=1.0):\n",
        "        return np.where(Z > 0, Z, alpha * (np.exp(Z) - 1))\n",
        "\n",
        "    @staticmethod\n",
        "    def delu(Z, alpha=1.0):\n",
        "        return np.where(Z > 0, 1, alpha * np.exp(Z))\n",
        "\n",
        "    @staticmethod\n",
        "    def selu(Z):\n",
        "        alpha = 1.67326\n",
        "        lam = 1.0507\n",
        "        return lam * np.where(Z > 0, Z, alpha * (np.exp(Z) - 1))\n",
        "\n",
        "    @staticmethod\n",
        "    def dselu(Z):\n",
        "        alpha = 1.67326\n",
        "        lam = 1.0507\n",
        "        return lam * np.where(Z > 0, 1, alpha * np.exp(Z))\n",
        "\n",
        "    @staticmethod\n",
        "    def softplus(Z):\n",
        "        return np.log1p(np.exp(Z))\n",
        "\n",
        "    @staticmethod\n",
        "    def dsoftplus(Z):\n",
        "        return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "    @staticmethod\n",
        "    def swish(Z, beta=1.0):\n",
        "        sig = 1 / (1 + np.exp(-beta * Z))\n",
        "        return Z * sig\n",
        "\n",
        "    @staticmethod\n",
        "    def dswish(Z, beta=1.0):\n",
        "        sig = 1 / (1 + np.exp(-beta * Z))\n",
        "        return sig + Z * beta * sig * (1 - sig)\n",
        "\n",
        "    @staticmethod\n",
        "    def gelu(Z):\n",
        "        # exact form: x * 0.5 * (1 + erf(x/sqrt(2)))\n",
        "        return Z * 0.5 * (1 + erf(Z / np.sqrt(2)))\n",
        "\n",
        "    @staticmethod\n",
        "    def dgelu(Z):\n",
        "        # derivative approximation can be implemented if needed\n",
        "        sig = 1 / (1 + np.exp(-Z))\n",
        "        return sig * (1 + Z * (1 - sig))  # simplified approx.\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(Z):\n",
        "        expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
        "        return expZ / expZ.sum(axis=0, keepdims=True)"
      ],
      "metadata": {
        "id": "YEj20I8iMzo_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LRScheduler:\n",
        "    \"\"\"Learning rate scheduler with basic decay strategies.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def fixed(lr0, step):\n",
        "        return lr0\n",
        "\n",
        "    @staticmethod\n",
        "    def time_decay(lr0, step, decay=1e-3):\n",
        "        return lr0 / (1 + decay * step)\n",
        "\n",
        "    @staticmethod\n",
        "    def step_decay(lr0, step, drop=0.5, epochs_drop=10):\n",
        "        return lr0 * drop**(step // epochs_drop)\n",
        "\n",
        "    @staticmethod\n",
        "    def exp_decay(lr0, step, decay=1e-2):\n",
        "        return lr0 * np.exp(-decay * step)\n",
        "\n",
        "class LRRL:\n",
        "    \"\"\"\n",
        "    Choose learning rate dynamically using a multi-armed bandit (e.g., Exp3).\n",
        "    Arms = discrete set of candidate learning rates.\n",
        "    Rewards = measured improvements during training \"episodes\".\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr_list, gamma=0.1):\n",
        "        self.lr_list = lr_list\n",
        "        self.K = len(lr_list)\n",
        "        self.weights = np.ones(self.K)\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def get_lr(self):\n",
        "        prob = (1 - self.gamma) * (self.weights / self.weights.sum()) + (self.gamma / self.K)\n",
        "        idx = np.random.choice(self.K, p=prob)\n",
        "        return self.lr_list[idx], idx\n",
        "\n",
        "    def update(self, idx, reward):\n",
        "        x = np.zeros(self.K); x[idx] = reward / self.weights[idx]\n",
        "        self.weights *= np.exp((self.gamma / self.K) * x)"
      ],
      "metadata": {
        "id": "3p8-qnKNOdUt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGDMomentum:\n",
        "    \"\"\"\n",
        "    Stochastic Gradient Descent with Momentum.\n",
        "    params: list of parameter arrays (e.g., [W1, b1, W2, b2])\n",
        "    grads: list of corresponding gradients\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, beta=0.9, scheduler=None):\n",
        "        self.lr = lr\n",
        "        self.beta = beta\n",
        "        self.scheduler = scheduler  # e.g., a learning rate schedule function\n",
        "        self.v = None\n",
        "        self.step = 0\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            # Initialize velocity terms with zeros\n",
        "            self.v = [np.zeros_like(p) for p in params]\n",
        "\n",
        "        # Retrieve current learning rate, possibly decayed\n",
        "        lr = self.scheduler(self.lr, self.step) if self.scheduler else self.lr\n",
        "\n",
        "        new_params = []\n",
        "        for i, (p, g) in enumerate(zip(params, grads)):\n",
        "            self.v[i] = self.beta * self.v[i] + (1 - self.beta) * g\n",
        "            new_p = p - lr * self.v[i]\n",
        "            new_params.append(new_p)\n",
        "\n",
        "        self.step += 1\n",
        "        return new_params"
      ],
      "metadata": {
        "id": "184vIaAtTyNn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSProp:\n",
        "    \"\"\"\n",
        "    RMSProp optimizer.\n",
        "    Maintains moving average of squared gradients and updates parameters adaptively.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.001, decay=0.9, epsilon=1e-8, scheduler=None):\n",
        "        self.lr = lr\n",
        "        self.decay = decay  # often referred to as 'rho'\n",
        "        self.epsilon = epsilon\n",
        "        self.scheduler = scheduler\n",
        "        self.s = None\n",
        "        self.step = 0\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.s is None:\n",
        "            self.s = [np.zeros_like(p) for p in params]\n",
        "\n",
        "        lr_t = self.scheduler(self.lr, self.step) if self.scheduler else self.lr\n",
        "\n",
        "        updated_params = []\n",
        "        for p, g, si in zip(params, grads, self.s):\n",
        "            si[:] = self.decay * si + (1 - self.decay) * (g ** 2)\n",
        "            updated_params.append(p - lr_t * g / (np.sqrt(si) + self.epsilon))\n",
        "\n",
        "        self.step += 1\n",
        "        return updated_params"
      ],
      "metadata": {
        "id": "tmwM3DILULkp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Adam:\n",
        "    \"\"\"\n",
        "    Adam optimizer combining momentum and RMSProp.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, scheduler=None):\n",
        "        \"\"\"\n",
        "        :param lr: learning rate (alpha)\n",
        "        :param beta1: momentum decay rate (first moment)\n",
        "        :param beta2: RMSProp decay rate (second moment)\n",
        "        :param epsilon: small constant to avoid division by zero\n",
        "        :param scheduler: optional learning-rate schedule function: fn(lr0, step) → lr_t\n",
        "        \"\"\"\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "        self.m = None  # First moment vector\n",
        "        self.v = None  # Second moment vector\n",
        "        self.step = 0\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m = [np.zeros_like(p) for p in params]\n",
        "            self.v = [np.zeros_like(p) for p in params]\n",
        "\n",
        "        self.step += 1\n",
        "        lr_t = self.scheduler(self.lr, self.step) if self.scheduler else self.lr\n",
        "\n",
        "        updated_params = []\n",
        "        for i, (p, g) in enumerate(zip(params, grads)):\n",
        "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * g\n",
        "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (g ** 2)\n",
        "\n",
        "            m_hat = self.m[i] / (1 - self.beta1 ** self.step)\n",
        "            v_hat = self.v[i] / (1 - self.beta2 ** self.step)\n",
        "\n",
        "            updated = p - lr_t * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "            updated_params.append(updated)\n",
        "\n",
        "        return updated_params"
      ],
      "metadata": {
        "id": "XgeqRkSgKzwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConjugateGradient:\n",
        "    \"\"\"\n",
        "    Nonlinear Conjugate Gradient optimizer for neural networks.\n",
        "    Requires:\n",
        "      - loss_fn(flat_params) → scalar loss\n",
        "      - grad_fn(flat_params) → flattened gradient\n",
        "    Uses Polak-Ribiere update:\n",
        "      d_t = -g_t + β_t d_{t-1}\n",
        "    Includes line search stub for step size α determination.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.prev_grad = None\n",
        "        self.prev_dir = None\n",
        "\n",
        "    def update(self, flat_params, loss_fn, grad_fn):\n",
        "        grad = grad_fn(flat_params)\n",
        "\n",
        "        if self.prev_grad is None:\n",
        "            direction = -grad\n",
        "        else:\n",
        "            y = grad - self.prev_grad\n",
        "            beta = np.dot(grad, y) / (np.dot(self.prev_grad, self.prev_grad) + 1e-12)  # Polak-Ribiere\n",
        "            direction = -grad + beta * self.prev_dir\n",
        "\n",
        "        # Line search to find optimal alpha (stub)\n",
        "        alpha = self.line_search(flat_params, direction, loss_fn)\n",
        "\n",
        "        new_flat_params = flat_params + alpha * direction\n",
        "\n",
        "        self.prev_grad = grad\n",
        "        self.prev_dir = direction\n",
        "\n",
        "        return new_flat_params\n",
        "\n",
        "    def line_search(self, flat_params, direction, loss_fn):\n",
        "        \"\"\"\n",
        "        Simple backtracking line search:\n",
        "        α = max α ∈ {cⁿ α0} such that loss decreases adequately.\n",
        "        \"\"\"\n",
        "        alpha = 1.0\n",
        "        c = 0.5\n",
        "        rho = 0.5\n",
        "        loss0 = loss_fn(flat_params)\n",
        "        grad_phi0 = np.dot(self.prev_grad, direction)\n",
        "        while True:\n",
        "            new_loss = loss_fn(flat_params + alpha * direction)\n",
        "            if new_loss <= loss0 + c * alpha * grad_phi0:\n",
        "                break\n",
        "            alpha *= rho\n",
        "            if alpha < 1e-6:\n",
        "                break\n",
        "        return alpha"
      ],
      "metadata": {
        "id": "e3PWIeqQLX3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PINNOptimizer:\n",
        "    \"\"\"\n",
        "    Physics-Informed Neural Network optimizer with gradient enhancement (gPINN).\n",
        "    Only uses finite differences – no autograd dependencies.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_opt, pinn_residual_fn, pinn_data, weight_pde=1.0, weight_grad=0.1, fd_eps=1e-4):\n",
        "        \"\"\"\n",
        "        :param base_opt: optimizer (SGD, Adam, etc.)\n",
        "        :param pinn_residual_fn: fn(params, X_f) -> residuals vector of shape (N_f,)\n",
        "        :param pinn_data: collocation points (X_f)\n",
        "        :param weight_pde: weight on residual loss\n",
        "        :param weight_grad: weight on gradient-of-residual loss\n",
        "        :param fd_eps: finite difference epsilon\n",
        "        \"\"\"\n",
        "        self.base = base_opt\n",
        "        self.res_fn = pinn_residual_fn\n",
        "        self.X_f = pinn_data\n",
        "        self.w_pde = weight_pde\n",
        "        self.w_grad = weight_grad\n",
        "        self.fd_eps = fd_eps\n",
        "\n",
        "    def pinn_res(self, params):\n",
        "        \"\"\"\n",
        "        Computes residual r(x) = u''(x) - f(x) at collocation points via finite differences.\n",
        "        \"\"\"\n",
        "        W1, b1, W2, b2 = params  # for a simple 1-hidden-layer network\n",
        "        def u(x):\n",
        "            z1 = W1.dot(x) + b1\n",
        "            a1 = np.tanh(z1)\n",
        "            return (W2.dot(a1) + b2).flatten()\n",
        "\n",
        "        x = self.X_f\n",
        "        u_plus = u(x + self.fd_eps)\n",
        "        u_minus = u(x - self.fd_eps)\n",
        "        u0 = u(x)\n",
        "        u_xx = (u_plus - 2*u0 + u_minus) / (self.fd_eps**2)\n",
        "        return u_xx - self.f(x)\n",
        "\n",
        "    def pinn_grads_fd(self, params):\n",
        "        \"\"\"\n",
        "        Finite-difference approximation of PDE residual gradients.\n",
        "        Approximates ∂(Σ r(x)^2)/∂θ for each param using finite difference.\n",
        "        \"\"\"\n",
        "        grads = []\n",
        "        r0 = self.res_fn(params, self.X_f)  # baseline residuals\n",
        "        N_f = r0.shape[0]\n",
        "        for W in params:\n",
        "            gW = np.zeros_like(W)\n",
        "            it = np.nditer(W, flags=['multi_index'], op_flags=['readwrite'])\n",
        "            while not it.finished:\n",
        "                idx = it.multi_index\n",
        "                orig = W[idx]\n",
        "                W[idx] = orig + self.fd_eps\n",
        "                r1 = self.res_fn(params, self.X_f)\n",
        "                W[idx] = orig - self.fd_eps\n",
        "                r2 = self.res_fn(params, self.X_f)\n",
        "                W[idx] = orig\n",
        "                # gradient of sum(residual^2) ≈ 2 * residual * dr/dw\n",
        "                dr = (r1 - r2) / (2 * self.fd_eps)\n",
        "                gW[idx] = np.sum(2 * r0 * dr)\n",
        "                it.iternext()\n",
        "            grads.append(gW)\n",
        "        return grads\n",
        "\n",
        "    def update(self, params, data_grads):\n",
        "        \"\"\"\n",
        "        Combine data-driven gradients with PDE residual & gradient-of-residual gradients,\n",
        "        then perform an optimizer step.\n",
        "        \"\"\"\n",
        "        pinn_grads = self.pinn_grads_fd(params)\n",
        "\n",
        "        combined = [\n",
        "            dg + self.w_pde * pg + self.w_grad * pg\n",
        "            for dg, pg in zip(data_grads, pinn_grads)\n",
        "        ]\n",
        "        return self.base.update(params, combined)\n"
      ],
      "metadata": {
        "id": "U1NZriYKLYse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HessianFreeOptimizer:\n",
        "    def __init__(self, model, loss_fn, max_iter=10, cg_iter=10, tol=1e-6):\n",
        "        \"\"\"\n",
        "        Initialize the Hessian-Free optimizer.\n",
        "\n",
        "        Parameters:\n",
        "        - model: The neural network model.\n",
        "        - loss_fn: The loss function.\n",
        "        - max_iter: Maximum number of outer iterations.\n",
        "        - cg_iter: Number of conjugate gradient iterations.\n",
        "        - tol: Tolerance for convergence.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.loss_fn = loss_fn\n",
        "        self.max_iter = max_iter\n",
        "        self.cg_iter = cg_iter\n",
        "        self.tol = tol\n",
        "\n",
        "    def compute_gradients(self, X, y):\n",
        "        \"\"\"\n",
        "        Compute the gradients of the loss function with respect to the model parameters.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input data.\n",
        "        - y: True labels.\n",
        "\n",
        "        Returns:\n",
        "        - gradients: Gradients of the loss function.\n",
        "        \"\"\"\n",
        "        # Forward pass\n",
        "        y_pred = self.model.forward(X)\n",
        "        # Compute loss\n",
        "        loss = self.loss_fn(y_pred, y)\n",
        "        # Backward pass\n",
        "        gradients = self.model.backward(X, y)\n",
        "        return gradients, loss\n",
        "\n",
        "    def compute_hvp(self, X, y, vector):\n",
        "        \"\"\"\n",
        "        Compute the Hessian-vector product using the conjugate gradient method.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input data.\n",
        "        - y: True labels.\n",
        "        - vector: Vector to compute the Hessian-vector product with.\n",
        "\n",
        "        Returns:\n",
        "        - hvp: Hessian-vector product.\n",
        "        \"\"\"\n",
        "        def hvp_fn(v):\n",
        "            # Compute the gradient of the loss with respect to the model parameters\n",
        "            gradients, _ = self.compute_gradients(X, y)\n",
        "            # Compute the Hessian-vector product\n",
        "            hvp = np.dot(gradients, v)\n",
        "            return hvp\n",
        "\n",
        "        # Use conjugate gradient method to compute the Hessian-vector product\n",
        "        hvp = self.conjugate_gradient(hvp_fn, vector)\n",
        "        return hvp\n",
        "\n",
        "    def conjugate_gradient(self, A, b):\n",
        "        \"\"\"\n",
        "        Solve Ax = b using the conjugate gradient method.\n",
        "\n",
        "        Parameters:\n",
        "        - A: Function that computes the matrix-vector product.\n",
        "        - b: Right-hand side vector.\n",
        "\n",
        "        Returns:\n",
        "        - x: Solution vector.\n",
        "        \"\"\"\n",
        "        x = np.zeros_like(b)\n",
        "        r = b - A(x)\n",
        "        p = r\n",
        "        rs_old = np.dot(r, r)\n",
        "        for _ in range(self.cg_iter):\n",
        "            Ap = A(p)\n",
        "            alpha = rs_old / np.dot(p, Ap)\n",
        "            x += alpha * p\n",
        "            r -= alpha * Ap\n",
        "            rs_new = np.dot(r, r)\n",
        "            if np.sqrt(rs_new) < self.tol:\n",
        "                break\n",
        "            p = r + (rs_new / rs_old) * p\n",
        "            rs_old = rs_new\n",
        "        return x\n",
        "\n",
        "    def step(self, X, y):\n",
        "        \"\"\"\n",
        "        Perform a single optimization step.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input data.\n",
        "        - y: True labels.\n",
        "        \"\"\"\n",
        "        # Compute the gradient and loss\n",
        "        gradients, loss = self.compute_gradients(X, y)\n",
        "        # Compute the Hessian-vector product\n",
        "        hvp = self.compute_hvp(X, y, gradients)\n",
        "        # Update the model parameters\n",
        "        self.model.update_parameters(hvp)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "MQ_F02-YQH6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseOptimizer:\n",
        "    def step(self, params, grads, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Unified interface for optimizer steps.\n",
        "        Subclasses or wrapped optimizers should implement this.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Optimizer must implement step().\")\n",
        "\n",
        "class UpdateAdapterOptimizer(BaseOptimizer):\n",
        "    def __init__(self, impl):\n",
        "        self.impl = impl\n",
        "\n",
        "    def step(self, params, grads, *args, **kwargs):\n",
        "        # Adapts your existing update(params, grads) signature\n",
        "        return self.impl.update(params, grads)\n",
        "\n",
        "class OptimizerFactory:\n",
        "    @staticmethod\n",
        "    def create(name, **kwargs):\n",
        "        name = name.lower()\n",
        "        if name == \"sgd\":\n",
        "            return UpdateAdapterOptimizer(SGDMomentum(**kwargs))\n",
        "        elif name == \"rmsprop\":\n",
        "            return UpdateAdapterOptimizer(RMSProp(**kwargs))\n",
        "        elif name == \"adam\":\n",
        "            return UpdateAdapterOptimizer(Adam(**kwargs))\n",
        "        elif name in (\"cg\", \"conjugategradient\"):\n",
        "            return ConjugateGradient()\n",
        "        elif name == \"pinn\":\n",
        "            base = kwargs.pop(\"base_opt\")\n",
        "            pinn_res = kwargs.pop(\"pinn_residual_fn\")\n",
        "            pinn_data = kwargs.pop(\"pinn_data\")\n",
        "            return UpdateAdapterOptimizer(\n",
        "                PINNOptimizer(base, pinn_res, pinn_data, **kwargs)\n",
        "            )\n",
        "        elif name in (\"hessianfree\", \"hf\"):\n",
        "            model = kwargs.pop(\"model\")\n",
        "            loss_fn = kwargs.pop(\"loss_fn\")\n",
        "            return HessianFreeOptimizer(model, loss_fn, **kwargs)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown optimizer: {name}\")"
      ],
      "metadata": {
        "id": "1aH790jaab5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelPersistence:\n",
        "    @staticmethod\n",
        "    def save(model, path):\n",
        "        with open(path,\"wb\") as f:\n",
        "            pickle.dump(model, f)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path):\n",
        "        with open(path,\"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "class NeuralNet:\n",
        "    def __init__(self, layers, activations, loss_fn, optimizer: BaseOptimizer):\n",
        "        self.layers = layers\n",
        "        self.activations = activations\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer  # Any object implementing step()\n",
        "        self.weights, self.biases = self._initialize_parameters()\n",
        "\n",
        "    def _initialize_parameters(self):\n",
        "        import numpy as np\n",
        "        Ws, bs = [], []\n",
        "        for i in range(len(self.layers)-1):\n",
        "            Ws.append(np.random.randn(self.layers[i], self.layers[i+1]))\n",
        "            bs.append(np.zeros((1, self.layers[i+1])))\n",
        "        return Ws, bs\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.zs, self.activations_cache = [], [X]\n",
        "        for W, b, act_fn in zip(self.weights, self.biases, self.activations):\n",
        "            z = self.activations_cache[-1] @ W + b\n",
        "            self.zs.append(z)\n",
        "            self.activations_cache.append(act_fn(z))\n",
        "        return self.activations_cache[-1]\n",
        "\n",
        "    def backward_logic(Z1, A1, A2, W2, X, Y_oh):\n",
        "        \"\"\"\n",
        "        Compute the gradients of the loss function with respect to weights and biases.\n",
        "\n",
        "        Parameters:\n",
        "        - Z1: Linear activation of the first layer\n",
        "        - A1: Activation of the first layer\n",
        "        - A2: Activation of the second layer (output)\n",
        "        - W2: Weights of the second layer\n",
        "        - X: Input data\n",
        "        - Y_oh: One-hot encoded true labels\n",
        "\n",
        "        Returns:\n",
        "        - grads_W: List of gradients for weights\n",
        "        - grads_b: List of gradients for biases\n",
        "        \"\"\"\n",
        "        m = X.shape[1]  # Number of examples\n",
        "\n",
        "        # Compute gradients for the second layer\n",
        "        dZ2 = A2 - Y_oh\n",
        "        dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
        "        db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "        # Compute gradients for the first layer\n",
        "        dA1 = np.dot(W2.T, dZ2)\n",
        "        dZ1 = dA1 * drelu(Z1)  # Assuming drelu is the derivative of ReLU\n",
        "        dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
        "        db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "        # Return gradients as lists\n",
        "        grads_W = [dW1, dW2]\n",
        "        grads_b = [db1, db2]\n",
        "        return grads_W, grads_b\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        \"\"\"\n",
        "        Perform the backward pass to compute gradients with respect to weights and biases.\n",
        "\n",
        "        Args:\n",
        "            X (numpy.ndarray): Input data of shape (input_size, batch_size).\n",
        "            y (numpy.ndarray): True labels of shape (output_size, batch_size).\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - grads_W (list): List of gradients with respect to weights for each layer.\n",
        "                - grads_b (list): List of gradients with respect to biases for each layer.\n",
        "        \"\"\"\n",
        "        # Compute gradients using the backpropagation logic\n",
        "        grads_W, grads_b = self.backward_logic(self.zs, self.activations_cache, y)\n",
        "\n",
        "        return grads_W, grads_b\n",
        "\n",
        "    def train_epoch(self, X, y):\n",
        "        params = self.get_flat_params()  # however you combine W & b\n",
        "        grads = self.get_flat_grads(X, y)\n",
        "        new_params = self.optimizer.step(params, grads)\n",
        "        self.set_flat_params(new_params)\n",
        "\n",
        "    def train(self, X, y, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            self.train_epoch(X, y)\n",
        "            if epoch % 10 == 0:\n",
        "                loss_val = self.loss_fn(self.forward(X), y)\n",
        "                print(f\"Epoch {epoch}, Loss {loss_val:.4f}\")\n",
        "\n",
        "class IncrementalTrainer:\n",
        "    def __init__(self, model: NeuralNet):\n",
        "        self.model = model\n",
        "\n",
        "    def update(self, new_X, new_y, epochs=10):\n",
        "        print(\"Updating with new data...\")\n",
        "        self.model.train(new_X, new_y, epochs)\n",
        "\n",
        "    def retrain(self, full_X, full_y, epochs=50):\n",
        "        print(\"Retraining from scratch...\")\n",
        "        self.model.weights, self.model.biases = self.model._initialize_parameters()\n",
        "        self.model.train(full_X, full_y, epochs)"
      ],
      "metadata": {
        "id": "ha2kc6SUUm_f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}